{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that the dataset has already been divided into a train set and a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOUSING_PATH = './datasets/cali_housing'\n",
    "TRAIN_PATH = os.path.join(HOUSING_PATH, 'train.csv')\n",
    "VALID_PATH = os.path.join(HOUSING_PATH, 'valid.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To normalize the data, we use the `keras.Normalization` layer, it can work in two ways, either you explicitly pass in the\n",
    "means and the stds for the data, or you pass in a data sample to its `adapt` method for it to calculate the values from \n",
    "the data sample. We choose the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_only_numeric_attribs(line):\n",
    "    \"\"\"\n",
    "    Processes only numeric attributes, which are subsequently\n",
    "    passed to the adapt() method of the Normalization layer\n",
    "    \"\"\"\n",
    "    defs = [0.] * 9 + [tf.constant([], dtype=tf.string)]\n",
    "    decoded = tf.io.decode_csv(line, defs)\n",
    "    X_numeric = tf.stack(decoded[:-2])\n",
    "    return X_numeric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numeric_attrib_dataset_factory(filepath):\n",
    "    dataset = tf.data.Dataset.list_files(filepath)\n",
    "    dataset = tf.data.TextLineDataset(dataset).skip(1)\n",
    "    dataset = dataset.map(preprocess_only_numeric_attribs)\n",
    "    return dataset.cache()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_attrib_dataset = numeric_attrib_dataset_factory(TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_layer = keras.layers.Normalization()\n",
    "norm_layer.adapt(numeric_attrib_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The means computed by the normalization layer's `adapt` method will be used to impute missing values, in this case, the total_bedrooms feature has missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_means = norm_layer.mean.numpy().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this preprocess function returns a total of 3 values, two of them packaged into a single tuple. This is because our model takes in two inputs, one of them being the 8 numerical values, and the other being a categorical input (string) - `ocean_proximity`,\n",
    "the keras layers handle encoding it using Embeddings as is shown later. Therefore we package those two into a single tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(line):\n",
    "    # Impute missing values with the means\n",
    "    defs =  x_means + [0.] + [tf.constant([], dtype=tf.string)]\n",
    "    decoded = tf.io.decode_csv(line, defs)\n",
    "    X_normal = tf.stack(decoded[:-2])\n",
    "    X_cat = tf.stack(decoded[-1:])\n",
    "    y = tf.stack(decoded[8:9])\n",
    "    return (X_normal, X_cat), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_reader_dataset(filepath):\n",
    "    dataset = tf.data.Dataset.list_files(filepath)\n",
    "    dataset = tf.data.TextLineDataset(dataset).skip(1)\n",
    "    dataset = dataset.map(preprocess)\n",
    "    return dataset.batch(32).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = csv_reader_dataset(TRAIN_PATH)\n",
    "valid_set = csv_reader_dataset(VALID_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This segment creates the vocabulary table, its `lookup` method takes in a string tensor and returns the category ie the index it belongs to, or returns an index that is out of bounds for the vocab list if it is out of vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats_vocab = ['<1H OCEAN', 'INLAND', 'NEAR OCEAN', 'NEAR BAY', 'ISLAND']\n",
    "num_oov_buckets = 2\n",
    "\n",
    "initializer = tf.lookup.KeyValueTensorInitializer(cats_vocab, tf.range(len(cats_vocab), dtype=tf.int64))\n",
    "vocab_table = tf.lookup.StaticVocabularyTable(initializer, num_oov_buckets=num_oov_buckets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is created using the functional api, as expected it has two inputs, one for the numeric attribs and the other for the categorical attribs, `ocean_proximity` in this case. The numeric inputs are subsequently passed into the normalization layer. The categorical inputs are first passed into a lambda layer to encode them ordinally, then they're passed into the Embedding layer to encode them using 10-dimensional embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 20\n",
    "normal_inputs = keras.layers.Input(shape=[8])\n",
    "normalized_normal_inputs = norm_layer(normal_inputs)\n",
    "cat_inputs = keras.layers.Input(shape=[], dtype=tf.string)\n",
    "\n",
    "cat_index_layer = keras.layers.Lambda(lambda cats: vocab_table.lookup(cats))(cat_inputs)\n",
    "embedding_layer = keras.layers.Embedding(len(cats_vocab) + num_oov_buckets, 10)(cat_index_layer)\n",
    "\n",
    "inputs_concatenated = keras.layers.concatenate([normalized_normal_inputs, embedding_layer])\n",
    "\n",
    "dense = keras.layers.Dense(\n",
    "    100, \n",
    "    activation='elu', \n",
    "    kernel_initializer='he_normal',\n",
    "    kernel_regularizer=keras.regularizers.L1(0.02)\n",
    ")(inputs_concatenated)\n",
    "\n",
    "bn_1 = keras.layers.BatchNormalization()(dense)\n",
    "\n",
    "for _ in range(n_hidden-1):\n",
    "    dense_2 = keras.layers.Dense(\n",
    "        100, activation='elu', \n",
    "        kernel_initializer='he_normal',\n",
    "        kernel_regularizer=keras.regularizers.L1(0.02)\n",
    "    )(dense)\n",
    "    bn = keras.layers.BatchNormalization()(dense_2)\n",
    "    dense = dense_2\n",
    "\n",
    "output = keras.layers.Dense(1)(dense_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Model(inputs=[normal_inputs, cat_inputs], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer='nadam', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_set, \n",
    "    epochs=100, \n",
    "    validation_data=valid_set, \n",
    "    callbacks=callbacks\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e3b4b67fb045f742fad44c0ba850b433822bc072bd46326ec254a62dfed69a68"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
